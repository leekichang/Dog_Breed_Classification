{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, cv2, random, time, shutil, csv\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "np.random.seed(42)\n",
    "%matplotlib inline \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Dense, GlobalAveragePooling2D, Lambda, Dropout, InputLayer, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "import gc\n",
    "import skimage.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../../data/train_keras/'\n",
    "train_labels = pd.read_csv('../../data/train_labels.csv')\n",
    "\n",
    "target, dog_breeds = pd.factorize(train_labels['breed'], sort = True)\n",
    "train_labels['target'] = target\n",
    "\n",
    "dog_breeds = sorted(list(set(train_labels['breed'])))\n",
    "n_classes = len(dog_breeds)\n",
    "\n",
    "class_to_num = dict(zip(dog_breeds, range(n_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_array(data_dir, labels_dataframe, img_size = (224,224,3)):\n",
    "    images_names = labels_dataframe['id']\n",
    "    images_labels = labels_dataframe['breed']\n",
    "    data_size = len(images_names)\n",
    "    #initailize output arrays.\n",
    "    X = np.zeros([data_size, img_size[0], img_size[1], img_size[2]], dtype=np.uint8)\n",
    "    y = np.zeros([data_size,1], dtype=np.uint8)\n",
    "    #read data and lables.\n",
    "    for i in tqdm(range(data_size)):\n",
    "        image_name = images_names[i]\n",
    "        img_dir = os.path.join(data_dir, image_name+'.jpg')\n",
    "        img_pixels = load_img(img_dir, target_size=img_size)\n",
    "        X[i] = img_pixels\n",
    "        \n",
    "        image_breed = images_labels[i]\n",
    "        y[i] = class_to_num[image_breed]\n",
    "    \n",
    "    y = to_categorical(y)\n",
    "    ind = np.random.permutation(data_size)\n",
    "    X = X[ind]\n",
    "    y = y[ind]\n",
    "    print('Ouptut Data Size: ', X.shape)\n",
    "    print('Ouptut Label Size: ', y.shape)\n",
    "    return X, y\n",
    "\n",
    "def get_features(model_name, data_preprocessor, input_size, data):\n",
    "    #Prepare pipeline.\n",
    "    input_layer = Input(input_size)\n",
    "    preprocessor = Lambda(data_preprocessor)(input_layer)\n",
    "    base_model = model_name(weights='imagenet', include_top=False,\n",
    "                            input_shape=input_size)(preprocessor)\n",
    "    avg = GlobalAveragePooling2D()(base_model)\n",
    "    feature_extractor = Model(inputs = input_layer, outputs = avg)\n",
    "    #Extract feature.\n",
    "    feature_maps = feature_extractor.predict(data, batch_size=128, verbose=1)\n",
    "    print('Feature maps shape: ', feature_maps.shape)\n",
    "    return feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20580 20580\n"
     ]
    }
   ],
   "source": [
    "img_size = (224,224,3)\n",
    "FOLDER = './'\n",
    "try:\n",
    "  X = np.load(FOLDER + \"X_save.npy\")\n",
    "  y = np.load(FOLDER + \"y_save.npy\")\n",
    "  print(len(X), len(y))\n",
    "except: \n",
    "  #img_size chosen to be 331 to suit the used architectures.\n",
    "  X, y = images_to_array(train_dir, train_labels, img_size)\n",
    "  np.save(FOLDER + \"X_save\", X)\n",
    "  np.save(FOLDER + \"y_save\", y)\n",
    "  \n",
    "train_X = X[:5000]\n",
    "train_y = y[:5000]\n",
    "val_X = X[5000:6000]\n",
    "val_y = y[5000:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 85s 2s/step\n",
      "Feature maps shape:  (5000, 2048)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "inceptionV3 = InceptionV3(weights='imagenet', include_top=False, input_shape=img_size)\n",
    "inception_features = get_features(InceptionV3,\n",
    "                                  inception_preprocessor,\n",
    "                                  img_size, train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"eff\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_23 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 7, 7, 1280)       4049571   \n",
      "                                                                 \n",
      " avg_pool (GlobalAveragePool  (None, 1280)             0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " batch_normalization_847 (Ba  (None, 1280)             5120      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " top_dropout (Dropout)       (None, 1280)              0         \n",
      "                                                                 \n",
      " pred (Dense)                (None, 2)                 2562      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,057,253\n",
      "Trainable params: 5,122\n",
      "Non-trainable params: 4,052,131\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "img_shape = img_size\n",
    "def build_model(base_model):\n",
    "    inputs = layers.Input(shape=img_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.4\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='eff')\n",
    "\n",
    "    return model\n",
    "\n",
    "inception_preprocessor = preprocess_input\n",
    "base_model = EfficientNetB0(include_top=False, input_shape=img_shape, weights=\"imagenet\")\n",
    "base_model.trainable = False\n",
    "model = build_model(base_model, 'eff')\n",
    "# Compile\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = keras.metrics.SparseCategoricalAccuracy()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=[metrics]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=5, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 1 must be sequence of length 2, not 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_77360/4103924850.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m              inner_model=model.get_layer(\"efficientnetb0\"))\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_stack_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../output/example_out\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Dog_Breed_Classification\\grad_Cam\\examples\\gradcam.py\u001b[0m in \u001b[0;36mgenerate_stack_img\u001b[1;34m(self, figsize, save_name, superimposed_name, alpha, cmap)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mimg_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0mimg_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_original\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0max1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Dog_Breed_Classification\\IIE4123\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2006\u001b[0m                 )\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 1 must be sequence of length 2, not 3"
     ]
    }
   ],
   "source": [
    "from gradcam import Gradcam\n",
    "\n",
    "img_path = \"../images/example.jpg\"\n",
    "\n",
    "gc = Gradcam(model, \n",
    "             layer_name=\"top_conv\",\n",
    "             img_path=img_path,\n",
    "             size=img_size,\n",
    "             inner_model=model.get_layer(\"efficientnetb0\"))\n",
    "\n",
    "gc.generate_stack_img(save_name=\"../output/example_out\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ee7f1d5d7a98b2a1e79ef1d845e0f49eec75467255a56e42fce5b0b6890b5c2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('IIE4123': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
