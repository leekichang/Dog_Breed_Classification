{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with cuda\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = './data/'\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "\n",
    "SaveModelName = \"vgg19\"\n",
    "\n",
    "ModelSavePath = \"./Saved_model/\" + SaveModelName + \"/\"\n",
    "if not os.path.isdir(ModelSavePath):\n",
    "    os.mkdir(ModelSavePath)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() \n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\") \n",
    "print(\"Working with\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET, TEST_SET, train_loader, test_loader = utils.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "model_transfer = models.vgg19(pretrained=False)\n",
    "n_dog_breed_classes = 120\n",
    "\n",
    "model_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for param in model_transfer.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_inputs = model_transfer.classifier[6].in_features\n",
    "\n",
    "last_layer = nn.Linear(n_inputs, n_dog_breed_classes)\n",
    "\n",
    "model_transfer.classifier[6] = last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ['vgg19']\n",
    "models = [model_transfer.to(DEVICE)]\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f35076e5931426799df99653d4d291a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EPOCHS:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch 001/10) Training Loss : 4.99016 | Validation Loss : 4.77383 | Validation Accuracy : 1.23 %\n",
      "(epoch 002/10) Training Loss : 4.77356 | Validation Loss : 4.75671 | Validation Accuracy : 1.47 %\n",
      "(epoch 003/10) Training Loss : 4.77654 | Validation Loss : 4.75552 | Validation Accuracy : 1.33 %\n",
      "(epoch 004/10) Training Loss : 4.77188 | Validation Loss : 4.75850 | Validation Accuracy : 1.25 %\n",
      "(epoch 005/10) Training Loss : 4.76431 | Validation Loss : 4.77405 | Validation Accuracy : 1.33 %\n",
      "(epoch 006/10) Training Loss : 4.77303 | Validation Loss : 4.75092 | Validation Accuracy : 1.65 %\n",
      "(epoch 007/10) Training Loss : 4.76165 | Validation Loss : 4.74081 | Validation Accuracy : 1.35 %\n",
      "(epoch 008/10) Training Loss : 4.75990 | Validation Loss : 4.74177 | Validation Accuracy : 1.50 %\n",
      "(epoch 009/10) Training Loss : 4.76035 | Validation Loss : 4.74029 | Validation Accuracy : 1.60 %\n",
      "(epoch 010/10) Training Loss : 4.75516 | Validation Loss : 4.72363 | Validation Accuracy : 1.47 %\n",
      "vgg19 Training took 1201.18 sec\n"
     ]
    }
   ],
   "source": [
    "LOSSES_TRAIN, LOSSES_VAL = [[] for idx in range(len(models))], [[] for idx in range(len(models))]\n",
    "ACCS_VAL = [[] for idx in range(len(models))]\n",
    "SOTA_ACCS_VAL, SOTA_LOSS_VAL = [0 for idx in range(len(models))], [0 for idx in range(len(models))]\n",
    "bestResult_pred_np, bestResult_anno_np = [[] for idx in range(len(models))], [[] for idx in range(len(models))]\n",
    "bestModels = [0 for idx in range(len(models))]\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    start = time.time()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    for epoch in tqdm(range(EPOCHS), desc=\"EPOCHS\"):\n",
    "        model.train()\n",
    "        LOSS_TRACE_FOR_TRAIN, LOSS_TRACE_FOR_VAL = [], []\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            X_train, Y_train = batch\n",
    "            X_train, Y_train = X_train.to(DEVICE), Y_train.to(DEVICE)\n",
    "\n",
    "            Y_pred_train = model(X_train)\n",
    "            Y_train = Y_train.squeeze(-1)\n",
    "\n",
    "            LOSS_train = criterion(Y_pred_train, Y_train)\n",
    "\n",
    "            LOSS_TRACE_FOR_TRAIN.append(LOSS_train.cpu().detach().numpy())\n",
    "            LOSS_train.backward()\n",
    "            optimizer.step()        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            Result_pred_val, Result_anno_val = [], []\n",
    "            for idx, batch in enumerate(test_loader):\n",
    "                X_val, Y_val = batch\n",
    "                X_val, Y_val = X_val.to(DEVICE), Y_val.to(DEVICE)\n",
    "\n",
    "                Y_pred_val = model(X_val)\n",
    "                LOSS_val = criterion(Y_pred_val, Y_val)\n",
    "                LOSS_TRACE_FOR_VAL.append(LOSS_val.cpu().detach().numpy())\n",
    "\n",
    "                Y_pred_val_np  = Y_pred_val.to('cpu').detach().numpy()\n",
    "                Y_pred_val_np  = np.argmax(Y_pred_val_np, axis=1).squeeze()\n",
    "                Y_val_np       = Y_val.to('cpu').detach().numpy().reshape(-1, 1).squeeze()     \n",
    "                \n",
    "                Result_pred_val = np.hstack((Result_pred_val, Y_pred_val_np))\n",
    "                Result_anno_val = np.hstack((Result_anno_val, Y_val_np))\n",
    "                # Result_pred_val.append(list(Y_pred_val_np))\n",
    "                # Result_anno_val.append(list(Y_val_np))\n",
    "            \n",
    "            Result_pred_np = np.array(Result_pred_val)\n",
    "            Result_anno_np = np.array(Result_anno_val)\n",
    "            Result_pred_np = np.reshape(Result_pred_np, (-1, 1))\n",
    "            Result_anno_np = np.reshape(Result_anno_np, (-1, 1))\n",
    "            \n",
    "            ACC_VAL        = metrics.accuracy_score(Result_anno_np, Result_pred_np)\n",
    "            AVG_LOSS_TRAIN = np.average(LOSS_TRACE_FOR_TRAIN)\n",
    "            AVG_LOSS_VAL   = np.average(LOSS_TRACE_FOR_VAL)\n",
    "\n",
    "            LOSSES_TRAIN[model_idx].append(AVG_LOSS_TRAIN)\n",
    "            LOSSES_VAL[model_idx].append(AVG_LOSS_VAL)\n",
    "            ACCS_VAL[model_idx].append(ACC_VAL)\n",
    "            \n",
    "            if ACC_VAL > SOTA_ACCS_VAL[model_idx]:\n",
    "                SOTA_ACCS_VAL[model_idx] = ACC_VAL\n",
    "                SOTA_LOSS_VAL[model_idx] = AVG_LOSS_VAL\n",
    "                bestModels[model_idx] = model\n",
    "                bestResult_pred_np[model_idx] = Result_pred_np\n",
    "                bestResult_anno_np[model_idx] = Result_anno_np\n",
    "                \n",
    "            elif ACC_VAL == SOTA_ACCS_VAL[model_idx]:\n",
    "                if AVG_LOSS_VAL < SOTA_LOSS_VAL[model_idx]:\n",
    "                    SOTA_ACCS_VAL[model_idx] = ACC_VAL\n",
    "                    SOTA_LOSS_VAL[model_idx] = AVG_LOSS_VAL\n",
    "                    bestModels[model_idx] = model\n",
    "                    bestResult_pred_np[model_idx] = Result_pred_np\n",
    "                    bestResult_anno_np[model_idx] = Result_anno_np\n",
    "                \n",
    "            print(f'(epoch {epoch + 1:03d}/{EPOCHS}) Training Loss : {AVG_LOSS_TRAIN:.5f} | Validation Loss : {AVG_LOSS_VAL:.5f} | Validation Accuracy : {ACC_VAL*100:.2f} %')\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"{model_name[model_idx]} Training took {end-start:.2f} sec\")\n",
    "    torch.save(bestModels[model_idx].state_dict(), ModelSavePath + model_name[model_idx] +'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bes1tResult_pred_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_73228/2910897945.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbes1tResult_pred_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manno\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbestResult_anno_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bes1tResult_pred_np' is not defined"
     ]
    }
   ],
   "source": [
    "utils.get_metrics(models = models, pred = bes1tResult_pred_np, anno = bestResult_anno_np)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "417d50cab6d0ee120e6de62fb3f5f413d1e73fa5d22c176bf88fbbd0170ec55f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
